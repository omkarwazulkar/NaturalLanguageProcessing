{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "N-Gram CNN Model Pro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rSLFdyI-5pvCN4T8gNALsGZBAcSJfD56",
      "authorship_tag": "ABX9TyNwjD83SPYOZfZaz7Xmsuvw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarwazulkar/NaturalLanguageProcessing/blob/main/N_Gram_CNN_Model_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from pickle import dump\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPubU_9aYVuQ",
        "outputId": "aee064b6-8855-43ed-cb2a-f673464eba92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Doc into Memory\n",
        "def load_doc(filename):\n",
        "  file = open(filename, 'r')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text"
      ],
      "metadata": {
        "id": "yJ84nniCYgDI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forming Clean Tokens\n",
        "def clean_doc(doc):\n",
        "  tokens = doc.split()\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "VzoBVKsAYk14"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading all documents in directory\n",
        "def process_docs(directory, is_train):\n",
        "  documents = list()\n",
        "  for filename in listdir(directory):\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "    path = directory + '/' + filename\n",
        "    doc = load_doc(path)\n",
        "    tokens = clean_doc(doc)\n",
        "    documents.append(tokens)\n",
        "  return documents"
      ],
      "metadata": {
        "id": "WJwa5Aj6Yur6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Clean\n",
        "def load_clean_dataset(is_train):\n",
        "  neg = process_docs('/content/drive/MyDrive/ReviewPopularity/txt_sentoken/neg', is_train)\n",
        "  pos = process_docs('/content/drive/MyDrive/ReviewPopularity/txt_sentoken/pos', is_train)\n",
        "  docs = neg + pos\n",
        "  labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "  return docs, labels"
      ],
      "metadata": {
        "id": "O3t1pZiwY9Xl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving to file\n",
        "def save_dataset(dataset, filename):\n",
        "  dump(dataset, open(filename, 'wb'))\n",
        "  print('Saved: %s' % filename)\n",
        "train_docs, ytrain = load_clean_dataset(True)\n",
        "test_docs, ytest = load_clean_dataset(False)"
      ],
      "metadata": {
        "id": "TatcG0lhwrNZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dataset([train_docs, ytrain], 'train.pkl')\n",
        "save_dataset([test_docs, ytest], 'test.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T32g2tcVx0E9",
        "outputId": "316c878f-fbae-4f4d-906c-72b069ac10d2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: train.pkl\n",
            "Saved: test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Developing Multi Channel Model**\n",
        "\n",
        "1.   Encoding\n",
        "2.   Defining Model\n",
        "\n"
      ],
      "metadata": {
        "id": "gHclKdwaZX8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate"
      ],
      "metadata": {
        "id": "TAltyHllZXnW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Clean Dataset\n",
        "def load_dataset(filename):\n",
        "  return load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "_jst6dwuZt2z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting Tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "pSGCdX6uaNdX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum Length of Document\n",
        "def max_length(lines):\n",
        "  return max([len(s.split()) for s in lines])"
      ],
      "metadata": {
        "id": "RW9n2Pa3aUqJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Encoding"
      ],
      "metadata": {
        "id": "ix-0pFg2ak5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding Lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "  encoded = tokenizer.texts_to_sequences(lines)\n",
        "  padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "  return padded"
      ],
      "metadata": {
        "id": "yqiEiJ4Qac3c"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Qu5-7M-UvrRp"
      },
      "outputs": [],
      "source": [
        "# Defining Model\n",
        "def define_model(length, vocab_size):\n",
        "\n",
        "  inputs1 = Input(shape=(length,))\n",
        "  embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "\n",
        "  inputs2 = Input(shape=(length,))\n",
        "  embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "  conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "\n",
        "  inputs3 = Input(shape=(length,))\n",
        "  embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "  conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "\n",
        "  dense1 = Dense(10, activation='relu')(merged)\n",
        "  outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "  \n",
        "  # Compling\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  plot_model(model, show_shapes=True, to_file='model.png')\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training and Saving Model**"
      ],
      "metadata": {
        "id": "AZEEaoe_a8VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Train Data\n",
        "trainLines, trainLabels = load_dataset('train.pkl')"
      ],
      "metadata": {
        "id": "5Y3k7vOcbErm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)"
      ],
      "metadata": {
        "id": "lsOjAuaPbIE3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Max Doc Lenght\n",
        "length = max_length(trainLines)\n",
        "print('Max document length: %d' % length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i7SSeeAbLEt",
        "outputId": "71174bef-163f-44c2-cebe-98fed5ecd8ed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 1380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary Size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8jV9vHubNIC",
        "outputId": "8934bd83-a08f-4902-b9dd-a6db01ff840c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 44277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "train_Labels = np.array(trainLabels)"
      ],
      "metadata": {
        "id": "MPZxHCV1bRul"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model\n",
        "model = define_model(length, vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSrVJ6pxbTGV",
        "outputId": "51d4ac39-19c8-4ae6-c779-ba93813618a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1380)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1380)]       0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1380)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 1380, 100)    4427700     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 1380, 100)    4427700     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 1380, 100)    4427700     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 1377, 32)     12832       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 1375, 32)     19232       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 1373, 32)     25632       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1377, 32)     0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 1375, 32)     0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 1373, 32)     0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 688, 32)      0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 687, 32)     0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 686, 32)     0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 22016)        0           ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 21984)        0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 21952)        0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 65952)        0           ['flatten[0][0]',                \n",
            "                                                                  'flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 10)           659530      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            11          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 14,000,337\n",
            "Trainable params: 14,000,337\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Model\n",
        "model.fit((trainX,trainX,trainX), train_Labels, epochs=7, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYOd4mGnbWmq",
        "outputId": "8762dcf7-97fa-4b68-8069-6e0b0bbb9288"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "113/113 [==============================] - 58s 495ms/step - loss: 0.6909 - accuracy: 0.5544\n",
            "Epoch 2/7\n",
            "113/113 [==============================] - 60s 535ms/step - loss: 0.4539 - accuracy: 0.7728\n",
            "Epoch 3/7\n",
            "113/113 [==============================] - 57s 504ms/step - loss: 0.0746 - accuracy: 0.9717\n",
            "Epoch 4/7\n",
            "113/113 [==============================] - 62s 547ms/step - loss: 0.0044 - accuracy: 0.9994\n",
            "Epoch 5/7\n",
            "113/113 [==============================] - 64s 569ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 6/7\n",
            "113/113 [==============================] - 62s 547ms/step - loss: 6.5206e-04 - accuracy: 1.0000\n",
            "Epoch 7/7\n",
            "113/113 [==============================] - 56s 493ms/step - loss: 3.8321e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fec811fe1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model\n",
        "model.save('model.h5')"
      ],
      "metadata": {
        "id": "VbpCY7gza2Qa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluating Model**"
      ],
      "metadata": {
        "id": "il-u5IU9brxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "nEnGARZAbwnA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Clean Data\n",
        "def load_dataset(filename):\n",
        "  return load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "qMh794Zvb4WW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting Tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "7UKqs5Snb6Bn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum Length of Document\n",
        "def max_length(lines):\n",
        "  return max([len(s.split()) for s in lines])"
      ],
      "metadata": {
        "id": "leOnFtvIb964"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding Lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "  encoded = tokenizer.texts_to_sequences(lines)\n",
        "  padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "  return padded"
      ],
      "metadata": {
        "id": "TdQWQqiacBgH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Train Data\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "testLines, testLabels = load_dataset('test.pkl')"
      ],
      "metadata": {
        "id": "Bl4Ruc3WcHGT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)"
      ],
      "metadata": {
        "id": "AxHYgA8JcOcg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Max Doc Lenght\n",
        "length = max_length(trainLines)\n",
        "print('Max document length: %d' % length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdfS53o5cRZJ",
        "outputId": "9dfadd94-6024-4f8c-c4a2-8b5bfda18698"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 1380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary Size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCASczLucaFm",
        "outputId": "4b5f97a9-c585-48e4-acfb-e638286c67e4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 44277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "testX = encode_text(tokenizer, testLines, length)"
      ],
      "metadata": {
        "id": "rSoswZSqcUFY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Model\n",
        "model = load_model('model.h5')"
      ],
      "metadata": {
        "id": "joxhRbe0cfTq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model on Train Data\n",
        "train_Labels = np.array(trainLabels)\n",
        "_, acc = model.evaluate([trainX,trainX,trainX], train_Labels, verbose=0)\n",
        "print('Train Accuracy: %.2f' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktb70a3MzJgh",
        "outputId": "3e009e35-39e0-4ecd-d3ca-525debb1b6eb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model on Test Dataset \n",
        "test_Labels = np.array(testLabels)\n",
        "_, acc = model.evaluate([testX,testX,testX], test_Labels, verbose=0)\n",
        "print('Test Accuracy: %.2f' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wq9Ei-pcnix",
        "outputId": "90dae53c-4786-4c19-ad94-1a88de210710"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 90.50\n"
          ]
        }
      ]
    }
  ]
}